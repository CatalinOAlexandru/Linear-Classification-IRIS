{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDSMbx-deCFZ"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLhwkD7YbOTY"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import rand"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmNEj3LXE9Zf"
      },
      "source": [
        "Dataset Loading from Local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VJzkLiwGph_"
      },
      "source": [
        "# Load Dataset from local Files\n",
        "\n",
        "# Load IRIS\n",
        "X_train_iris = np.load(\"iris_train_samples.npy\")\n",
        "Y_train_iris = np.load(\"iris_train_labels.npy\")\n",
        "X_val_iris = np.load(\"iris_val_samples.npy\")\n",
        "Y_val_iris = np.load(\"iris_val_labels.npy\")\n",
        "\n",
        "\n",
        "# Load MNIST\n",
        "X_train_mnist = np.load(\"mnist_train_samples.npy\")\n",
        "Y_train_mnist = np.load(\"mnist_train_labels.npy\")\n",
        "X_val_mnist = np.load(\"mnist_val_samples.npy\")\n",
        "Y_val_mnist = np.load(\"mnist_val_labels.npy\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dasq2etFFTdw"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysqo1K_RaFnt"
      },
      "source": [
        "\n",
        "def nsolve(X,y):\n",
        "  a = np.linalg.inv(np.dot(X.T,X))\n",
        "  w = np.dot(np.dot(a,X.T),y)\n",
        "  return w\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZPmK0WPJWer"
      },
      "source": [
        "## 3A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaKbJGnbaGnf"
      },
      "source": [
        "# Question 3-A\n",
        "\n",
        "def lclass(exampleA, exampleB, testExamples):\n",
        "    \n",
        "    # -------- PROCESSING DATA --------\n",
        "\n",
        "\n",
        "    # Creating the label matrix y\n",
        "    # ExampleA is going to be label 0\n",
        "    # ExampleB is going to be label 1\n",
        "    # We are doing this to differentiate the classes\n",
        "    y = []\n",
        "    for _ in range(exampleA.shape[0]):\n",
        "      y.append(0)\n",
        "    for _ in range(exampleB.shape[0]):\n",
        "      y.append(1)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Merging together exampleA and B to calculate theta\n",
        "    X = np.concatenate((exampleA,exampleB))\n",
        "    # Normalizing data between 0 and 1\n",
        "    X = (X-np.min(X))/(np.max(X)-np.min(X))\n",
        "    # Adding bias 1 in front of the X\n",
        "    X = np.array([np.append(1,i) for i in X])\n",
        "\n",
        "    # Normalizing data between 0 and 1\n",
        "    valX = (testExamples-np.min(testExamples))/(np.max(testExamples)-np.min(testExamples))\n",
        "    # Adding bias 1 in front of the valY\n",
        "    valX = np.array([np.append(1,i) for i in valX])\n",
        "\n",
        "\n",
        "    \n",
        "    # -------- CALCULATION THETA --------\n",
        "\n",
        "    theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "\n",
        "    # -------- PREDICTION --------\n",
        "\n",
        "    # Prediction will receive values of 0 and 1\n",
        "    # 0 means that values from testExamples are in exampleA\n",
        "    # 1 means that values from testExamples are in exampleB\n",
        "    prediction = (np.matmul(valX, theta) > 0.5).astype(np.int64)\n",
        "\n",
        "    return prediction\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LETCgjnJYiB"
      },
      "source": [
        "## 3B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shi_Mu69s5Wq"
      },
      "source": [
        "The next cell was taken from LAB 3 Classification as it was permitted on Moodle Forum in order to corrently represent the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCW6tCO0s3nd"
      },
      "source": [
        "def true_positive(Y_test, y_pred):\n",
        "    mask = (Y_test == 1)\n",
        "    \n",
        "    tp = (Y_test[mask] == y_pred[mask]).sum()\n",
        "    \n",
        "    return tp.item()\n",
        "\n",
        "def true_negative(Y_test, y_pred):\n",
        "    mask = (Y_test == 0) | (Y_test == -1)\n",
        "    \n",
        "    tn = (Y_test[mask] == y_pred[mask]).sum()\n",
        "    \n",
        "    return tn.item()\n",
        "\n",
        "def false_negative(Y_test, y_pred):\n",
        "    mask = (y_pred == 0) | (y_pred == -1)\n",
        "    \n",
        "    tn = (Y_test[mask] != y_pred[mask]).sum()\n",
        "    \n",
        "    return tn.item()\n",
        "\n",
        "def false_positive(Y_test, y_pred):\n",
        "    mask = (y_pred == 1)\n",
        "    \n",
        "    tn = (Y_test[mask] != y_pred[mask]).sum()\n",
        "    \n",
        "    return tn.item()\n",
        "\n",
        "def plot_confusion_matrix(Y_test, y_pred):\n",
        "    \n",
        "    tp = true_positive(Y_test, y_pred)\n",
        "    tn = true_negative(Y_test, y_pred)\n",
        "    fp = false_positive(Y_test, y_pred)\n",
        "    fn = false_negative(Y_test, y_pred)\n",
        "    \n",
        "    cf = np.array([[tn, fn], [fp, tp]])\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    ax.matshow(cf, cmap=plt.cm.Blues)\n",
        "\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            c = cf[j,i]\n",
        "            ax.text(i, j, str(c), va='center', ha='center')\n",
        "\n",
        "    plt.xlabel('Prediction')\n",
        "    plt.ylabel('Target')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6f2O9wItGCP"
      },
      "source": [
        "Question 3B\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVv9zXlWJZuH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "39eb7f8d-cb41-4145-e545-23a20de3e173"
      },
      "source": [
        "# Question 3-B\n",
        "\n",
        "# -------- SETTING UP THE DATA --------\n",
        "\n",
        "# Create empty arrays\n",
        "A = [] #exampleA\n",
        "B = [] #exampleB\n",
        "\n",
        "# Go through each label and split the data which has label 0 and anything else\n",
        "# This is in order to test Setosa vs Non-Setosa\n",
        "for i in range(Y_train_iris.shape[0]):\n",
        "    if Y_train_iris[i] == 0:\n",
        "      A.append(X_train_iris[i])\n",
        "    else:\n",
        "      B.append(X_train_iris[i])\n",
        "\n",
        "# convert to Numpy arrays\n",
        "exampleA = np.array(A)\n",
        "exampleB = np.array(B)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------- TESTING 3B --------\n",
        "\n",
        "# Testing with all IRIS samples\n",
        "result = lclass(exampleA, exampleB, X_val_iris)\n",
        "\n",
        "print(\"Validation set shape: \",Y_val_iris.shape)\n",
        "print(\"Result shape: \",result.shape)\n",
        "print(\"Result values: \\n\",result)\n",
        "print(\"Ground truth (Y_val_iris): \\n\",Y_val_iris)\n",
        "\n",
        "\n",
        "\n",
        "# -------- PLOTTING --------\n",
        "\n",
        "# As the prediction outputs 0 and 1, 0 to confirm that the testExample is in\n",
        "# the examplesA, and 1 to say that is not, I'm going to modify the ground truth\n",
        "# (Y_val_iris) to show only 0s and 1s.\n",
        "# This is because the ground truth hold 0s, 1s and 2s and the confusion matrix\n",
        "# will show the many values ass wrong.\n",
        "\n",
        "\n",
        "newValY = [] \n",
        "\n",
        "for i in range(Y_val_iris.shape[0]):\n",
        "    if Y_val_iris[i] == 0:\n",
        "      newValY.append(0)\n",
        "    else:\n",
        "      newValY.append(1)\n",
        "\n",
        "newValY = np.array(newValY)\n",
        "print(\"New Ground Truth with 0s and 1s only (read comments lines 39-43) \\n\",newValY)\n",
        "print()\n",
        "\n",
        "\n",
        "plot_confusion_matrix(newValY, result)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set shape:  (15,)\n",
            "Result shape:  (15,)\n",
            "Result values: \n",
            " [0 0 1 1 1 1 1 1 0 0 1 1 0 1 1]\n",
            "Ground truth (Y_val_iris): \n",
            " [0 0 2 2 1 2 2 1 0 0 2 2 0 2 1]\n",
            "New Ground Truth with 0s and 1s only (read comments lines 39-43) \n",
            " [0 0 1 1 1 1 1 1 0 0 1 1 0 1 1]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ugYF8wOyEM_"
      },
      "source": [
        "**Analysis**\n",
        "\n",
        "In the results above I firstly present the shapes of the label Validation set and the output results from the lclass function. We can see that both have the same size which means the output coresponds correctly to the number of features in the validation set.\n",
        "\n",
        "Next I present the actual output as 0s & 1s. The ground truth confirms that all Setosas labels (notes with 0 as a label) were correctly classfied. As describe in the task the 0 in the results stands for being part of the exampleA and 1 for not being part.\n",
        "\n",
        "Due to how the actual labels are presented, I re-built the validation set to easily represent the confusion matrix. All Setosas were labeled as 0 as before and anything else (in our case the other 2 class) with 1.\n",
        "\n",
        "In the end the confusion matrix (between the result and the new built validation set) confirms that 5 True Positives (5 Setosa classified correctly) were classified and 10 True Negatives (other classes which are not Setosa)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axPOy6OtJaB9"
      },
      "source": [
        "## 3C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN3mVDcq8QB5"
      },
      "source": [
        "# Question 3-C\n",
        "# examples - N-th examples / Features sets\n",
        "# clas - vector of labels of the same size\n",
        "# testExamples - test with this\n",
        "# return - a matrix, where each row expresses the probability of a sample in \n",
        "#          testExamples to belong to each class\n",
        "\n",
        "\n",
        "# Linear Multi-class classification using One-VS-Rest Method (More in 3D)\n",
        "def lmclass(examples, clas, testExamples):\n",
        "    \n",
        "    # Used to know how many different classes of labels we have in the dataset\n",
        "    unique, counts = np.unique(clas, return_counts=True)\n",
        "\n",
        "    # to store multiple results later\n",
        "    results = []\n",
        " \n",
        "    # loop for each uniqu label. In IRIS we loop 3 times. For each iteration we\n",
        "    # get the classification for:\n",
        "    # Setosa vs (Versicolour,Virginica)\n",
        "    # Versicolour vs (Setosa,Virginica)\n",
        "    # Virginica vs (Setosa,Versicolour)\n",
        "    for l in range(len(unique)):\n",
        "\n",
        "      # -------- PROCESSING DATA --------\n",
        "      a = [] # to be the first class to be classfied\n",
        "      b = [] # to be the rest of classes which we classify against\n",
        "      y = [] # used for the new revised labels \n",
        "\n",
        "      # loop for each label present in the dataset\n",
        "      for i in range(clas.shape[0]):\n",
        "        # if the label at i is the same as the current unique label we save in a\n",
        "        # the features coresponding to this label and we also save a label 0 as\n",
        "        # this is the main class vs label 1 the rest of classes.\n",
        "        if clas[i] == unique[l]:\n",
        "          a.append(examples[i])\n",
        "          y.append(0)\n",
        "        else:\n",
        "          b.append(examples[i])\n",
        "          y.append(1)\n",
        "\n",
        "      # converting to numpy arrays\n",
        "      exampleA = np.array(a)\n",
        "      exampleB = np.array(b)\n",
        "      y = np.array(y)\n",
        "\n",
        "      \n",
        "      # -------- PREDICTION --------\n",
        "      # We predict using the equation from 3A\n",
        "      prediction = multiClass(exampleA, exampleB, testExamples)\n",
        "      # We append the vector of 0s and 1s into the results\n",
        "      results.append(prediction)\n",
        "\n",
        "\n",
        "    # After all iteration finished, we return the new matrix which contains all\n",
        "    # the predictions for each classification.\n",
        "    results = np.array(results)\n",
        "    return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-21gpkDJbwt"
      },
      "source": [
        "## 3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olWuIG8kt8GI"
      },
      "source": [
        "The function multiClass is a copy of the 3A but slightly modified to show as a probability. \n",
        "\n",
        "Next cells contain the task 3D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRV-Tb41rgqQ"
      },
      "source": [
        "def multiClass(exampleA, exampleB, testExamples):\n",
        "    y = []\n",
        "    for _ in range(exampleA.shape[0]):\n",
        "      y.append(0)\n",
        "    for _ in range(exampleB.shape[0]):\n",
        "      y.append(1)\n",
        "    y = np.array(y)\n",
        "    X = np.concatenate((exampleA,exampleB))\n",
        "    X = (X-np.min(X))/(np.max(X)-np.min(X))\n",
        "    X = np.array([np.append(1,i) for i in X])\n",
        "    valX = (testExamples-np.min(testExamples))/(np.max(testExamples)-np.min(testExamples))\n",
        "    valX = np.array([np.append(1,i) for i in valX])\n",
        "\n",
        "    theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "    prediction = (np.matmul(valX, theta))\n",
        "    # Everything above is as in 3A\n",
        "\n",
        "    # From here is slightly modified\n",
        "\n",
        "    # Normalizing the output to be between 0 and 1\n",
        "    predNorm = (prediction-np.min(prediction))/(np.max(prediction)-np.min(prediction))\n",
        "    # rounding to 2 decimals\n",
        "    predRounded = (np.round(predNorm,decimals=2))\n",
        "\n",
        "    return predRounded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H18VvG7xkuOM"
      },
      "source": [
        "Q-3D\n",
        "\n",
        "Source: https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\n",
        "\n",
        "After reserching multi-class classification I found that is not an easy solution due to its linearity and not being able to draw a straigh light thought 3 classes and properly devide them. By reading the article from Machine Learning Mastery, there are 2 solutions/methods which can solve the multi-class problem. \n",
        "\n",
        "One of these is One-VS-One which classifies a class vs another and later puts the results together.\n",
        "\n",
        "The 2nd solution which I personally used is One-Vs-Rest which does the linear classification by using a class and putting the other classes together as the 2nd. I found this more useful as It shows clearly if a test is in the single class or somewhere in the rest of classes.\n",
        "\n",
        ".\n",
        "\n",
        "Now seeing the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZGzRH_3Jc6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c9230b-a7d8-4fb2-8192-a85be15d9e89"
      },
      "source": [
        "# Question 3-D\n",
        "\n",
        "# -------- RESULTS --------\n",
        "results = lmclass(X_train_iris,Y_train_iris,X_val_iris)\n",
        "\n",
        "print(\"Ground truth (Y_val_iris): \\n\",Y_val_iris)\n",
        "\n",
        "resultsBinary = (results > 0.5).astype(np.int64)\n",
        "print(\"Binary Results: \\n\",resultsBinary)\n",
        "\n",
        "print(\"Probabiliy Results: \\n\",results)\n",
        "print(\"\\n \\n\")\n",
        "\n",
        "\n",
        "correlation = round(np.corrcoef(Y_val_iris,results)[0,1],2)\n",
        "print(\"Correlation is: \",correlation)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ground truth (Y_val_iris): \n",
            " [0 0 2 2 1 2 2 1 0 0 2 2 0 2 1]\n",
            "Binary Results: \n",
            " [[0 0 1 1 1 1 1 1 0 0 1 1 0 1 1]\n",
            " [1 1 0 1 1 1 0 0 1 1 1 1 1 1 1]\n",
            " [1 1 0 0 1 0 0 1 1 1 0 0 1 0 1]]\n",
            "Probabiliy Results: \n",
            " [[ 0.13 -0.    1.46  1.05  0.89  0.96  1.26  0.91  0.2   0.18  1.    0.94\n",
            "  -0.03  1.18  0.72]\n",
            " [ 0.78  0.88  0.28  0.72  0.55  0.66  0.26  0.47  0.58  0.65  0.75  0.77\n",
            "   0.97  0.59  0.59]\n",
            " [ 1.09  1.12  0.26  0.23  0.56  0.38  0.48  0.62  1.22  1.18  0.25  0.28\n",
            "   1.06  0.23  0.7 ]]\n",
            "\n",
            " \n",
            "\n",
            "Correlation is:  0.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_YKRoZpp2CZ"
      },
      "source": [
        "In the output above we can the ground truth and next the binary representation of the results. In the binary representation Row 1 is for Setosa (label 0), row 2 for Versicolour (label 1) and row 3 for Virginica (label 2).\n",
        "\n",
        "Were is a 0 in binary means it was correctly classified and where is a 1 means that was not (it was part of the \"rest\" dataset).\n",
        "\n",
        "We can see that row 2, column 3 we have a 0 where it should be a 1, that is one missclassification so we are not 100% accurate. We can see this in the correlation which is 0.94.\n",
        "\n",
        "\n",
        "Probability to be correctly classified is also shown in the \"Probability results\""
      ]
    }
  ]
}